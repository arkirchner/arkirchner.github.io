---
title: "Host your applications on GKE!"
date: 2020-06-24T16:01:16+09:00
categories:
  - GKE
tags:
  - Minikube
  - Helm
  - Google Cloud
  - K8s
---

Use Google Kubernetes Engine (GKE) to host your applications. This guide gives 
you a framework to deploy your applications. First we look at the deployment 
with a local cluster. Then move the GKE. With a little trick we use only one GKE 
load balancer for all out applications and safe some money.

# Prepare your local machine

In this study session, we will need a couple of tools.

## Required tools:
- **kubectl**
- **Minikube**
- **Helm3**
- **Google Cloud SDK**

To tests automatic certificate creation for your applications you need a domain.

### Let's start with installing kubectl
The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters.
> If you are not using Homebrew please see alternative installation methods [here](https://kubernetes.io/docs/tasks/tools/install-kubectl/).

If you get an error please follow the install instrucktions for **kubectl**.
```shell
brew install kubectl

kubectl version --client
```

### Install and test Minikube for local testing
> If you are not using Homebrew please see alternative installation methods [here](https://kubernetes.io/docs/tasks/tools/install-minikube/).

Minikube is a single-node Kubernetes cluster in a virtual machine on your computer. On your mac it will use HyperKit to run a virtual machine.

```shell
brew install minikube
```

Let's test if minikube is working porperly by opening the Kubernetes dashboard.
> Minikube will start downloading virtual machine images. This can take some time...

```shell
minikube start
minikube dashboard
# stop with CTRL+C
```

After you have confirmed that everything is working well please stop and delete your cluster for now.

```
minikube stop
minikube delete
```

### Let's add Helm3 for the Ngnix proxy
> If you are not using Homebrew please see alternative installation methods [here](https://helm.sh/docs/intro/install/).

```shell
brew install helm

helm version
>version.BuildInfo{Version:"v3.2.2", GitCommit:"a6ea66349ae3015618da4f547677a14b9ecc09b3", GitTreeState:"clean", GoVersion:"go1.13.12"}
```

### Install the Google SDK

First please follow the instructions to install the Google SDK.
[https://cloud.google.com/sdk/docs/downloads-interactive](https://cloud.google.com/sdk/docs/downloads-interactive)

I recommend creating a new project for this tutorial.

```shell
gcloud auth
gcloud projects create your-project-id --name="Study project"
```

Please enable billing for this project. We need to create billable resources.
I will go into deleting all billed resource at the end of this guide.
[https://console.cloud.google.com/billing](https://console.cloud.google.com/billing)

# Let's start with creating an application locally

## First let's start Minikube
```shell
minikube start
kubectl cluster-info
> Kubernetes master is running at https://192.168.64.3:8443
> KubeDNS is running at https://192.168.64.3:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

## Creating a Deployment configuration for your application

In Kubernetis you deploy an application with a [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment). At Deployment is a configuration object for your Pods and the ReplicaSets.

Let's deploy this example. Please create a file `deplyment.yml` and copy the configuration.

```yaml
# deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foo
  labels:
    app: foo
spec:
  selector:
    matchLabels:
      app: foo
  template:
    metadata:
      labels:
        app: foo
    spec:
      containers:
      - name: foo
        image: gcr.io/google-samples/hello-app:1.0
        ports:
        - name: http
          containerPort: 8080
```

Let's deploy this application to our local cluster. In Kubernetes, you **apply** configurations to the cluster.
 
```shell
kubectl apply -f deployment.yml
> deployment.apps/foo created
```

Confirm if the deployment was successful. 
```shell
kubectl get deployments
> NAME   READY   UP-TO-DATE   AVAILABLE   AGE
> foo    1/1     1            1           52s

kubectl get pods
> NAME                   READY   STATUS    RESTARTS   AGE
> foo-6f8947bdb8-hpvpn   1/1     Running   0          4m38s
```

We can see that our application **foo** is **Ready 1/1**.

We can see the random name given to our POD **foo-6f8947bdb8-hpvpn**.  The pod names are very dynamic. For example, with every update, this name will change.

Let's see our deployment in the browser. With **kubectl** we can forward ports of our application POD to our local PC.

```shell
kubectl port-forward pods/foo-6f8947bdb8-hpvpn 8080:http
# Close with CTRL+C
```

Let's look at our application in the browser:

[http://localhost:8080/](http://localhost:8080/) 

You should see a hello world page with the name of the POD:
```text
Hello, world!
Version: 1.0.0
Hostname: foo-6f8947bdb8-hpvpn
```

Next, let's scale up our application to 3 replicas. Please add **`replicas: 3`** to our `deployment.yml` file.
```yaml
...
spec:
+ replicas: 3
  selector:
...
```

Let's update our application with **kubectl** and see the output for our **Deployment** and **Pods**.

```shell
kubectl apply -f deployment.yml
> deployment.apps/foo configured

kubectl get deployments
> NAME   READY   UP-TO-DATE   AVAILABLE   AGE
> foo    3/3     3            3           142m

kubectl get pods
> NAME                   READY   STATUS    RESTARTS   AGE
> foo-6f8947bdb8-gmwvd   1/1     Running   0          10m
> foo-6f8947bdb8-hpvpn   1/1     Running   0          146m
> foo-6f8947bdb8-sshvp   1/1     Running   0          10m
``` 

You should see all three replicas of our app running. Please see the two new PODs also got a randomly generated name.

## Create a single endpoint for our application

At this moment we have only `kubectl port-forward` to expose individual PODs on our local PC. It would be much better if we would have a load balancer on our Kubernetes cluster that would expose all 3 PODs on a single endpoint.  Luckily Kubernetes offers us this kind of load balancers. They are called **Service**. 

Let's deploy a service for our deployment. We will use the service of **`type: NodePort`**. This kind of service will expose a port on our Minikube server. The range of valid ports for a NodePort is 30000-32767. A random port will be selected.

```yaml
# service.yml
apiVersion: v1
kind: Service
metadata:
  name: foo
spec:
  type: NodePort
  selector:
    app: foo
  ports:
  - name: http
    port: 80
    targetPort: http
```

Let's apply our service to the cluster.
```shell
kubectl apply -f service.yml
> service/foo created
```

Let's see if our service is running and what port was assigned.
```shell
kubectl get services
> NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
> foo          NodePort    10.96.112.172   <none>        80:30211/TCP   2m18s
> kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        4h33m
```
> The service kubernetes is created by Minikube.

We should  see our service with the name **foo** is created and what port is exposed **`80:EXPOSED_PORT/TCP`**. In my case port **30211** is exposed. Please note that a diffrent port was opened for you.

To access our application through the service we need to know the IP of our Minikube server.

```bash
minikube ip
> 192.168.64.3
```
> The IP will be diffrent on your PC!

Now let's combine the IP and PORT and curl our new service a couple of times. Please note how the **Hostname** is changing when a different **POD** receives our request.

```shell
curl http://192.168.64.3:30211
> Hello, world!
> Version: 1.0.0
> Hostname: foo-6f8947bdb8-hpvpn

curl http://192.168.64.3:30211
> Hello, world!
> Version: 1.0.0
> Hostname: foo-6f8947bdb8-gmwvd

curl http://192.168.64.3:30211
> Hello, world!
> Version: 1.0.0
> Hostname: foo-6f8947bdb8-sshvp
```

So far so good. We have a scalable and load-balanced application on our Minikube server.

## Let's expose port 80 and route by domain

With the service of `type: NodePort` we can expose our service to the network. However, we are limited in the ports we can use. It would be much better if we can expose **port 80** to the web. It would be good if the can serve more the one application on one port by assigning a **host like `foo.example`** to our application.

Kubernetes offers an **Ingress** resource to do just that. With an **Ingress** HTTP/HTTPS traffic can be routed to a service. Traffic routing is controlled by rules defined on the Ingress resource.

**Ingress** are not like a **Pod** or **Service** or **Deployment**. There is no default **Ingress**. Every cloud provider has implemented the **Ingress** in its own way. There are many open source **Ingress** controller implementations.

Minikube comes with an **Nginx** based **Ingress**. Let's activate it!

```shell
minikube addons enable ingress
>ðŸŒŸ  The 'ingress' addon is enabled
```

Let's make sure our ingress is listening on port 80 of our Minikube server.
> `minikube ip` will tell you the IP of your minikube server.

```shell
curl http://192.168.64.3
> <html>
> <head><title>404 Not Found</title></head>
> <body>
> <center><h1>404 Not Found</h1></center>
> <hr><center>nginx/1.17.10</center>
> </body>
> </html>
```

If you get a 404 error from Nginx everything is working fine.

Let's create our Ingress resource.

```
# ingess.yml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: foo
spec:
  rules:
  - host: foo.example
    http:
      paths:
      - path: /
        backend:
          serviceName: foo
          servicePort: http
```

Apply our new **Ingress** resource to the cluster.
```shell
kubectl apply -f ingress.yml
> ingress.networking.k8s.io/foo created
```

Let's see if our **Ingress** resource was deployed correctly.
```shell
kubectl get ingress
> NAME   CLASS    HOSTS         ADDRESS        PORTS   AGE
> foo    <none>   foo.example   192.168.64.3   80      2m45s
```

Let's curl our **Ingress** resource.
```shell
curl --header "Host: foo.example" http://192.168.64.3
> Hello, world!
> Version: 1.0.0
> Hostname: foo-6f8947bdb8-gmwvd
```

It looks like it is working! Let's add an entry in our `/etc/hosts` file to see our app in the browser. Please add some more test domains for later.

```
# /etc/hosts
192.168.64.3 foo.example bar.example baz.example
```

Now, let's open our application in the browser!

> http://foo.example

## Let's change our service type

We still have a port open on our service of **`type: NodePort`**. We do not need to expose this port to the outside anymore. Kubernetes offers another type of service that is responsible for internal networking. It is called **ClusterIP**. This type is the default Kubernetes service type.

First, let's delete our current service because the service type can not be changed for an existing service.

```shell
kubectl delete -f service.yml
> service "foo" deleted
```

Change our service configuration to ClusterIP.
> It is OK to just remove the type since ClusterIP is the default type. 
```yml
# service.yml
apiVersion: v1
kind: Service
metadata:
  name: foo
spec:
-  type: NodePort
+  type: ClusterIP
  selector:
    app: foo
  ports:
  - name: http
    port: 80
    targetPort: http
```

Apply our updated service of type ClusterIP.

```shell
kubectl apply -f service.yml
> service/foo created
```

Let's see our service one more time.
```shell
kubectl get service
> NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
> foo          ClusterIP   10.103.55.219   <none>        80/TCP    2m55s
> kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   17h
```

We can see that we are no longer exposing a port to the outside. Our service type is now ClusterIP.

Let's visit [http://foo.example](http://foo.example) to see that everything is still working.

Note:
You can expose the internal service of ClusterIP to your local machine through port forwarding.

```shell
kubectl port-forward services/foo 8080:http
# CTRL + C to cancel
```

## Let's move everything into a namespace
 
Namespaces are virtual clusters backed by the same physical cluster. Applications in one namespace can not communicate with applications in another namespace. Namespaces provide a safe way to separate applications from each other.

To start let's delete our Deployment, Service, and Ingress in the default namespace.

```shell
kubectl delete -f ingress.yml
> ingress.networking.k8s.io "foo" deleted

kubectl delete -f service.yml
> service "foo" deleted

kubectl delete -f deployment.yml
> deployment.apps "foo" deleted
```

Have a look at what namespaces we are currently having in our Minikube cluster.

```shell
kubectl get namespace
> NAME              STATUS   AGE
> default           Active   27h
> kube-node-lease   Active   27h
> kube-public       Active   27h
> kube-system       Active   27h
```


Currently, we have 3 namespaces beginning with **kube-**. These namespaces are used to run Kubernetes itself. **default** is the namespace we have been deploying our applications to until now.

Let's create a namespace for our Application.

You can create a namespace with a configuration file just like we did before with the other parts of our application.

```yaml
# namespace.yml
apiVersion: v1
kind: Namespace
metadata:
  name: foo
```

Apply our changes to our cluster.
```shell
kubectl apply -f namespace.yml
> namespace/foo created

kubectl get namespace
> NAME              STATUS   AGE
> default           Active   28h
> foo               Active   2m27s
> kube-node-lease   Active   28h
> kube-public       Active   28h
> kube-system       Active   28h
```

Update the configuration of our **Deployment**, **Service**, and **Ingress** to use our new namespace.

```yml
# deplyment.yml service.yml ingress.yml
...
metadata:
  name: foo
+ namespace: foo
  labels:
...
```

Now let's deploy our application into our new namespace.

```shell
kubectl apply -f deployment.yml
> deployment.apps/foo created

kubectl apply -f service.yml
> service/foo created

kubectl apply -f ingress.yml
> ingress.networking.k8s.io/foo created
```

Let's see the status of our application in the namespace **foo**.

```shell
kubectl get all --namespace foo
> NAME                       READY   STATUS    RESTARTS   AGE
> pod/foo-6f8947bdb8-2dpr2   1/1     Running   0          5m23s
> pod/foo-6f8947bdb8-ft9bz   1/1     Running   0          5m23s
> pod/foo-6f8947bdb8-rxc44   1/1     Running   0          5m23s
> 
> NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
> service/foo   ClusterIP   10.99.153.149   <none>        80/TCP    2m47s
> 
> NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
> deployment.apps/foo   3/3     3            3           5m24s
> 
> NAME                             DESIRED   CURRENT   READY   AGE
> replicaset.apps/foo-6f8947bdb8   3         3         3       5m23s
```

> Let's check [http://foo.example](http://foo.example)

After this let's delete our **namespace foo** and with it everything in the namespace.

```shell
kubectl delete namespace foo
> namespace "foo" deleted
```

# Packaging our application with Helm
Helm helps you manage Kubernetes applications â€” Helm Charts help you define, install, and upgrade even the most complex Kubernetes application.

To understand how Helm works we will use it to create a Chart of our application.

Let's generate our Chart.

```shell
helm create simple_app
#=> Creating simple_app
```


Let's remove the example configuration that comes with `helm create` and remove all current values.

```shell
rm -f simple_app/templates/**/*
rm simple_app/values.yaml
touch simple_app/values.yaml
```

Next, let's copy our **deployment.yml**, **service.yml** and **ingress.yml** into the `simple_app/templates/` folder.

{% raw %}
Now let's make our application dynamic by replacing all **foo**s with **{{ .Release.Name }}** and our host **foo.example** with **{{ .Values.domain }}**.

```yaml
# simple_app/templates/deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
  labels:
    app: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replicas }}
  selector:
    matchLabels:
      app: {{ .Release.Name }}
  template:
    metadata:
      labels:
       app: {{ .Release.Name }}
    spec:
      containers:
      - name: {{ .Release.Name }}
        image: gcr.io/google-samples/hello-app:1.0
        ports:
        - name: http
          containerPort: 8080
```


```yaml
# simple_app/templates/service.yml
apiVersion: v1
kind: Service
metadata:
  name: {{ .Release.Name }}
spec:
  type: ClusterIP
  selector:
    app: {{ .Release.Name }}
  ports:
  - name: http
    port: 80
    targetPort: http
```

```yaml
# simple_app/templates/ingress.yml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: {{ .Release.Name }}
spec:
  rules:
  - host: {{ .Values.domain }}
    http:
      paths:
      - path: /
        backend:
          serviceName: {{ .Release.Name }}
          servicePort: http
```

Now add the following values into our **value.yaml**.

```yaml
# simple_app/values.yaml
domain: foo.example
replicas: 2
```

Install our application with Helm a couple of times:

```shell
helm install --set domain=foo.example \
             --set replicas=4 \
             --create-namespace \
             --namespace=foo \
             foo \
             ./simple_app

helm install --set domain=bar.example \
             --set replicas=8 \
             --create-namespace \
             --namespace=bar \
             bar \
             ./simple_app



helm install --set domain=baz.example \
             --set replicas=2 \
             --create-namespace \
             --namespace=baz \
             baz \
             ./simple_app
```

See if all our applications are running...

> [http://foo.example](http://foo.example)
> [http://bar.example](http://bar.example)
> [http://baz.example](http://baz.example)

This concludes building our application deployment framework.


# Deploy the application on GKE

Before we start let's not forget to stop Minikube.

```shell
minikube stop
# minikube delete (if you want to remove the VM)
```

## What will our application look like?
```
  Namespace
+----------------------------------------------------------------+
|                                                                |
|  +---------------+     +-----------------+     +------------+  |
|  |  Ingress      |     | Service         |     | Deployment |  |
|  |  + class: gce +---->+ + type NodePort +---->+ + App Foo  |  |
|  +--++-----------+     +-+---+-----------+     +-+----------+  |
|      ^                       ^                                 |
|      |                       |                                 |
|  +---+------------------+  +-+---------------------+           |
|  | Certificate          |  | Backend Configuration |           |
|  | + domains            |  | + Cloud CDN           |           |
|  |   + foo.example.com  |  |   + Enabled           |           |
|  |   + bar.example.com  |  | etc ...               |           |
|  |                      |  |                       |           |
|  +----------------------+  +-----------------------+           |
|                                                                |
+----------------------------------------------------------------+
```

## Setup our GKE environment.
Now start with the Google Cloud. Please log into the [Google Cloud Console](https://console.cloud.google.com)

First make sure we are using the correct project.
```shell
gcloud config set project your-project-id
> Updated property [core/project].


# Confirm the current project is selected.
gcloud config get-value project
> your-project-id
```

The first component we need it an IP address to access out cluster.


```shell
gcloud compute addresses create primary-ip --global
> Created [https://www.googleapis.com/compute/v1/projects/your-project-id/global/addresses/primary-ip].

gcloud compute addresses describe --global primary-ip
> address: 34.102.150.60
> addressType: EXTERNAL
> creationTimestamp: '2020-06-22T18:21:23.713-07:00'
> description: ''
> id: '277616844101969916'
> ipVersion: IPV4
> kind: compute#address
> name: primary-ip
> networkTier: PREMIUM
> selfLink: https://www.googleapis.com/compute/v1/projects/your-project-id/global/addresses/primary-ip
> status: RESERVE
```

If you have access to a domain please go ahead and point your DNS settings to the created IP.
Please send me the address IP that was assigned to yous


Now to the exciting part. Let's create our GKE cluster for our apps by executing the following command.

```shell
gcloud container clusters create \
  --machine-type e2-small \
  --num-nodes 1 \
  --zone us-central1-f \
  --cluster-version latest \
  --preemptible \
  study-cluster
> kubeconfig entry generated for study-cluster.
> NAME           LOCATION       MASTER_VERSION  MASTER_IP        MACHINE_TYPE  > NODE_VERSION  NUM_NODES  STATUS
> study-cluster  us-central1-f  1.16.9-gke.6    104.197.228.240  e2-small      1.16.9-gke.6  1          RUNNING
```
> Please be aware that your kubectl configuration has been changed to the GKE cluster. It is no longer pointing to Minikube!

Let's release an application to our cluster. Please copy the configuration below.

```yaml
# gke_app.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nginx-ingress
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foo
  namespace: nginx-ingress
  labels:
    app: foo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: foo
  template:
    metadata:
      labels:
       app: foo
    spec:
      containers:
      - name: foo
        image: gcr.io/google-samples/hello-app:1.0
        ports:
        - name: http
          containerPort: 8080
---
# apiVersion: cloud.google.com/v1
# kind: BackendConfig
# metadata:
#   namespace: nginx-ingres
#   name: primary-backend-config
# spec:
#   cdn:
#     enabled: true
#     cachePolicy:
#       includeHost: true
#       includeProtocol: true
#       includeQueryString: false
---
apiVersion: v1
kind: Service
metadata:
  name: foo
  namespace: nginx-ingress
  #  annotations:
  #    cloud.google.com/backend-config: '{"ports": {"80":"primary-backend-config"}}'
spec:
  type: NodePort
  selector:
    app: foo
  ports:
  - name: http
    port: 80
    targetPort: http
---
# apiVersion: networking.gke.io/v1beta2
# kind: ManagedCertificate
# metadata:
#   namespace: nginx-ingress
#   name: primary
# spec:
#   domains:
#     - foo.example.com
#     - bar.example.com
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: primary
  namespace: nginx-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: primary-ip
    kubernetes.io/ingress.class: gce
    # networking.gke.io/managed-certificates: primary
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: foo
          servicePort: http
```

Deploy all to your GKE cluster with the following command.

```shell
kubectl apply -f gke-app.yml

> namespace/nginx-ingress created
> deployment.apps/foo created
> service/foo created
> ingress.networking.k8s.io/primary created
```

While our GKE cluster is starting our application let's talk about some changes I made to our Minikube configuration.

#### Changes in the Ingress
```yaml
  rules:
( - host: foo.example)
  - http:
  ...
  annotations:
    kubernetes.io/ingress.global-static-ip-name: primary-ip
    kubernetes.io/ingress.class: gce
```
I removed the host configuration. This allows us to see our app with just the IP.

I added two annotations. The first annotation tells the Google Ingress what IP to use. I gave it the name of the global IP we created earlier. The Google Ingress will connect the LoadBalancer it this IP address.

The second annotation tells Kubernetes what Ingress class it should use.  GKE comes with an Ingress Controller preconfigured (class: gce). Later I want to add a second Ingress Controller. For that is very important to set the **Ingress.Class** to prevent both Ingress controllers fighting over the same Ingress object.

#### Changes to the Service
```yaml
spec:
  type: NodePort
```

I change the service to be a **NodePort** again. This is due to the implementation of the GCE Ingress. The GCE Ingress is not installed directly into the Kubernetes cluster like the Ngnix Ingress on our Minikube cluster. Instead, it uses a Compute Engine LoadBalancer outside of the cluster. With our GKE Ingress configuration, we are telling Google to connect this LoadBalancer to the open port of our **NodePort Service**. The only way to let external services connect the cluster is the services of type **NodePort**.   

#### Why creating a Namespace for Nginx
```yaml
  namespace: nginx-ingress
```

I want to save time. The configuration or the GCE Loadbalancer takes time. In the next step, we will create an **alternative Ingress Controller** with the **`class: nginx`**. 

### OK it is time to check the status of our deployment.

```shell
kubectl get all --namespace=nginx-ingress
> NAME                       READY   STATUS    RESTARTS   AGE
> pod/foo-5df8cf56f8-9qk4x   1/1     Running   0          21m
> pod/foo-5df8cf56f8-n2nmg   1/1     Running   0          21m
> 
> NAME          TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
> service/foo   NodePort   10.11.252.1   <none>        80:32289/TCP   21m
> 
> NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
> deployment.apps/foo   2/2     2            2           21m
> 
> NAME                             DESIRED   CURRENT   READY   AGE
> replicaset.apps/foo-5df8cf56f8   2         2         2       21m
```

Not lets look at our app in the browser.
> http://34.102.150.60
> Please use your IP here.
 
### Enable GKE specific features.

Let's have a look at the ManagedCertificate object that Google provides. You can tell it is a Google configuration by looking at the **`apiVersion: networking.gke.io/v1beta2`** (gke.io).

Let's uncomment the certificate and annotation.
```
apiVersion: networking.gke.io/v1beta2
kind: ManagedCertificate
metadata:
  name: primary
  namespace: nginx-ingress
spec:
  domains:
    - foo.example.com # Please update with your own domain
    - bar.example.com
```

Let's tell Ingress about the certificate by uncommenting the annotation. The name must match with the **ManagedCertificate** name.
```yaml
-    # networking.gke.io/managed-certificates: primary
+    networking.gke.io/managed-certificates: primary
```

Apply the changes.
```shell
kubectl apply -f gke-app.yml
> namespace/nginx-ingress unchanged
> deployment.apps/foo unchanged
> service/foo unchanged
> managedcertificate.networking.gke.io/primary created
> ingress.networking.k8s.io/primary configured
```

This change takes a while...

In the next step, I want to show you how easy it is to used Google services like CloudCDN or CloudAmore with GKE. Again we have a Google configuration for this. This time we need to apply this to our service.

Please uncomment the **BackendConfig**.
```yaml
apiVersion: cloud.google.com/v1
kind: BackendConfig
metadata:
  namespace: nginx-ingres
  name: primary-backend-config
spec:
  cdn:
    enabled: true
    cachePolicy:
      includeHost: true
      includeProtocol: true
      includeQueryString: false
```

Please uncomment the service annotation.
```yaml
-  #  annotations:
-  #    cloud.google.com/backend-config: '{"ports": {"80":"primary-backend-config"}}'
+  annotations:
+    cloud.google.com/backend-config: '{"ports": {"80":"primary-backend-config"}}'
```

Apply the changes.
```shell
kubectl apply -f gke-app.yml
> namespace/nginx-ingress unchanged
> deployment.apps/foo unchanged
> backendconfig.cloud.google.com/primary-backend-config created
> service/foo configured
> managedcertificate.networking.gke.io/primary unchanged
> ingress.networking.k8s.io/primary unchanged
```

With this small changes we have activated the CDN with cache configuration.

# Let us install our own Ingress
In the previous step, we have successfully deployed our application to GKE. Our application runs isolated in its one Namespace. The application is connected to the internet with a Google LoadBalancer including certificates and a CDN configuration.

To deploy more applications like this we can just copy this configuration and change the names.

However, there is a financial issue with this approach. Every new GCE Ingress we create will create a new LoadBalancer. Creating a separate LoadBalancer per application is very expensice.

So we need to think if we can somehow use one LoadBalancer for all our apps.

One option is to put everything in one Namespace. Then we can add one rule for every app in the Namespace. This approval is probably the easiest in the beginning but can turn out to be a mess with many applications.

The best way would be to find a way to deploy our applications into its own Namespace but still use only one GCP LoadBalancer.

Luckily there is a way to do that by creating a second Ingress controller in sequence to our GKE Ingress.

## Networking with two Ingress controllers on GKE
In our examples using Minikube we enabled an Ingress controller. This controller is an open-source project by the Kubernetes team.

> https://kubernetes.github.io/ingress-nginx/

This Ngnix Controller will create a **Service** and a **Deployment** running **Nginx**. Ngnix will create networking rules to **Ingresses of class: nginx** into other **Namespaces** thought some cluster rules.

If the **Service** of the **Nginx Ingress Controller** is of type **NodePort** we can route our traffic from the **GKE Ingress** directly to the **Nginx Ingress Controller**.  
 
```ascii
 namespcae: nginx-ingess
+------------------------------------------------------------------------+
|                                                                        |
|  +---------------+     +-----------------+     +--------------------+  |
|  |  Ingress      |     | Service         |     | Deployment         |  |
|  |  + class: gce +---->+ + type NodePort +---->+ + Nginx Controller |  |
|  +--++-----------+     +-+---+-----------+     +-+--------+---------+  |
|      ^                       ^                            |            |
|      |                       |                            |            |
|  +---+------------------+  +-+---------------------+      |            |
|  | Certificate          |  | Backend Configuration |      |            |
|  | + domains            |  | + Cloud CDN           |      |            |
|  |   + foo.example.com  |  |   + Enabled           |      |            |
|  |   + bar.example.com  |  | etc                   |      |            |
|  |                      |  |                       |      |            |
|  +----------------------+  +-----------------------+      |            |
|                                                           |            |
+------------------------------------------------------------------------+
                                                            |
                                                            |
                 +------------------------------------------+
                 |                                          |
                 |                                          |
 namespace: foo  v                       namespace: bar     v
+----------------+---------------+      +-------------------+------------+
|                                |      |                                |
|  +-------------------------+   |      |  +-------------------------+   |
|  |  Ingress                |   |      |  |  Ingress                |   |
|  |  + class: nginx         |   |      |  |  + class: nginx         |   |
|  |    domain: foo.example  |   |      |  |    domain: bar.example  |   |
|  +-----------+-------------+   |      |  +-----------+-------------+   |
|              |                 |      |              |                 |
|              |                 |      |              |                 |
|              v                 |      |              v                 |
|     +--------+----------+      |      |     +--------+----------+      |
|     | Service           |      |      |     | Service           |      |
|     | + type: ClusterIP |      |      |     | + type: ClusterIP |      |
|     |                   |      |      |     |                   |      |
|     +--------+----------+      |      |     +--------+----------+      |
|              |                 |      |              |                 |
|              |                 |      |              |                 |
|              v                 |      |              v                 |
|      +-------+---------+       |      |      +-------+---------+       |
|      | Deployment      |       |      |      | Deployment      |       |
|      | + Foo Dummy App |       |      |      | + Bar Dummy App |       |
|      |                 |       |      |      |                 |       |
|      +-----------------+       |      |      +-----------------+       |
|                                |      |                                |
+--------------------------------+      +--------------------------------+
```

## Install Nginx Ingress Controller with Helm
Like we did with our **simple_app Chart** the Nginx Ingress team has created a Helm chart to make the installation of the **Nginx Ingress** Controller easy.

We need to make sure that we set the following parameters:
- Service of type NodePort
- Service annotation for our Google backend configuration.
- Service only with port 80. Not necessary, I want to make clear that HTTPS is terminated at the GCE LoadBalancer.

Luckily the Helm Chart of Ngnix Ingress gives us some configuration options.

 > https://github.com/kubernetes/ingress-nginx/tree/master/charts/ingress-nginx

Let's install the Ingress Controller with correct settings for GKE.

```shell
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx/
helm install --namespace nginx-ingress \
             --set controller.service.type=NodePort \
             --set controller.service.enableHttps=false \
             --set controller.service.annotations.cloud\\.google\\.com/backend-config=\\{\"ports\":\\{\"80\":\"primary-backend-config\"\\}\\} \
             nginx-ingress \
             ingress-nginx/ingress-nginx
```

Let's see what is installed.
```shell
kubectl get all --namespace=nginx-ingress
> NAME                                                          READY   > > > STATUS    RESTARTS   AGE
> pod/foo-5df8cf56f8-7wsld                                      1/1     > Running   0          103m
> pod/foo-5df8cf56f8-lvsmr                                      1/1     Running   0          103m
> pod/nginx-ingress-ingress-nginx-controller-79d6d964c6-wfdmk   1/1     Running   0          69s

> NAME                                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
> service/foo                                                NodePort    10.11.255.174   <none>        80:32407/TCP   103m
> service/nginx-ingress-ingress-nginx-controller             NodePort    10.11.246.211   <none>        80:30870/TCP   70s
> service/nginx-ingress-ingress-nginx-controller-admission   ClusterIP   > 10.11.252.49    <none>        443/TCP        70s
> 
> NAME                                                     READY   UP-TO-DATE   AVAILABLE   AGE
> deployment.apps/foo                                      2/2     2            2           103m
> deployment.apps/nginx-ingress-ingress-nginx-controller   1/1     1            1           71s
> 
> NAME                                                                DESIRED   CURRENT   READY   AGE
> replicaset.apps/foo-5df8cf56f8                                      2         2         2       103m
> replicaset.apps/nginx-ingress-ingress-nginx-controller-79d6d964c6   1         1         1       71s
```

We can see the service of the ingress controller of type **NodePort**. The name is `nginx-ingress-ingress-nginx-controller`. Let's setup our **Google Ingress** controller to use this service as a backend.

```yaml
# gke-app.yml
         backend:
-          serviceName: foo
+          serviceName: nginx-ingress-ingress-nginx-controller
           servicePort: http
```

Let's update our GKE app deployment.
```shell
kubectl apply -f gke-app.yml
> namespace/nginx-ingress unchanged
> deployment.apps/foo unchanged
> backendconfig.cloud.google.com/primary-backend-config unchanged
> service/foo unchanged
> managedcertificate.networking.gke.io/primary unchanged
> ingress.networking.k8s.io/primary configured
```

When we visit our pages again.
```
# Error: Server Error

## The server encountered a temporary error and could not complete your request.

Please try again in 30 seconds.
```

Our service has, unfortunately, become unhealthy. This is because the Nginx controller is returning a 404 error page. However, the Nginx controller offers a health check path `/healthz` 

```shell
kubectl describe deployment nginx-ingress-ingress-nginx-controller --namespace=nginx-ingress

> ...
>     Liveness:   http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
>     Readiness:  http-get http://:10254/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
> ...
```

> Let's update healthcheck on [https://console.cloud.google.com](https://console.cloud.google.com).


## Deploy our simple_app Chart with Nginx Ingress

Befor we can do that we need to ensure we use the Ingress of **`type: nginx`**.

Lets update the templet.
```yaml
 # simple_app/templates/ingress.yml 
 kind: Ingress
 metadata:
   name: {{ .Release.Name }}
+  annotations:
+    kubernetes.io/ingress.class: nginx
```

OK we are ready to deploy our application into separate namespaces.

```shell
helm install --set domain=foo.example.com \
             --set replicas=2 \
             --create-namespace \
             --namespace=foo \
             foo \
             ./simple_app

helm install --set domain=bar.example.com \
             --set replicas=2 \
             --create-namespace \
             --namespace=bar \
             bar \
             ./simple_app
```

Done!!! Let's check out the test pages to make sure it worked!!!

I hope this guide did show you a framework deploy your projects with Kubernetis.

Please don't forget to delete the resource when you are done.

# Delete everything on GKE

The best way is to delete the whole project. Then everything is just gone.

> https://console.cloud.google.com/iam-admin/projects

In the project list, select the project that you want to delete and then click **Delete**  _delete_.

In the dialog, type the project ID and then click **Shut down** to delete the project.

You can also remove the cluster and the global IP address we created.

Remove the cluster:
```shell
gcloud container clusters delete study-cluster
> The following clusters will be deleted.
>  - [study-cluster] in [us-central1-f]
>
> Do you want to continue (Y/n)?  Y
> 
> Deleting cluster study-cluster...done.
> Deleted [https://container.googleapis.com/v1/projects/your-project-id/zones/us-central1-f/clusters/study-cluster].
```

Remove the IP:
```shell
gcloud compute addresses delete --global primary-ip
> The following global addresses will be deleted:
>  - [primary-ip]
> 
> Do you want to continue (Y/n)?  Y
> 
> Deleted [https://www.googleapis.com/compute/v1/projects/your-project-id/global/addresses/primary-ip].
```

{% endraw %}
